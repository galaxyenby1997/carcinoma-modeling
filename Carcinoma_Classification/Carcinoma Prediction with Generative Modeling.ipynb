{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was pulled from the UCI Respository and is a REAL dataset\n",
    "go to http://archive.ics.uci.edu/ml/datasets/HCC+Survival for more information!\n",
    "\n",
    "Abstract: Hepatocellular Carcinoma dataset (HCC dataset) \n",
    "was collected at a University Hospital in Portugal. \n",
    "It contains real clinical data of 165 patients diagnosed with HCC. This project will be \n",
    "an interesting look into doing classification with pretty small data, only 165 points.\n",
    "With almost 50 features (dimensions), this is a hard problem!\n",
    "\n",
    "It's also a good problem on tackling datasets that are small, as doing maximum likelihood\n",
    "estimation on small data can give results that might not be flexible for future prediction, while a Bayesian inference will give the whole distribution to play with and do some analysis. The Bayesian contet also lets us put some prior information (even if it isn't so strong), which can give us an edge over maximum likelihood when we don't have a lot of data, especially disease data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rabeya/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# First, we read in the file and import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc3 as pm\n",
    "from sklearn.metrics import confusion_matrix as cmatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HCC data\n",
    "hcc_data = pd.read_csv('/Users/rabeya/Carcinoma_Classification_data/hcc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.Gen</th>\n",
       "      <th>2.Sym</th>\n",
       "      <th>3.Alc</th>\n",
       "      <th>4.HepB</th>\n",
       "      <th>5.HepB</th>\n",
       "      <th>6.HepB</th>\n",
       "      <th>7.HepC</th>\n",
       "      <th>8.Cir</th>\n",
       "      <th>9.End</th>\n",
       "      <th>10.Smo</th>\n",
       "      <th>...</th>\n",
       "      <th>41.Alk</th>\n",
       "      <th>42.Prot</th>\n",
       "      <th>43.Crea</th>\n",
       "      <th>44.NNod</th>\n",
       "      <th>45.dnod</th>\n",
       "      <th>46.Bil</th>\n",
       "      <th>47.Iro</th>\n",
       "      <th>48.Oxy</th>\n",
       "      <th>49.Fer</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>?</td>\n",
       "      <td>59</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>280</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>181</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.46</td>\n",
       "      <td>5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>170</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>462</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3.95</td>\n",
       "      <td>5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>19.8</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1.Gen 2.Sym  3.Alc 4.HepB 5.HepB 6.HepB 7.HepC  8.Cir 9.End 10.Smo  ...  \\\n",
       "0        1     0      1      0      0      0      0      1     0      1  ...   \n",
       "1        0     ?      0      0      0      0      1      1     ?      ?  ...   \n",
       "2        1     0      1      1      0      1      0      1     0      1  ...   \n",
       "3        1     1      1      0      0      0      0      1     0      1  ...   \n",
       "4        1     1      1      1      0      1      0      1     0      1  ...   \n",
       "..     ...   ...    ...    ...    ...    ...    ...    ...   ...    ...  ...   \n",
       "160      0     0      1      ?      ?      ?      1      1     0      1  ...   \n",
       "161      0     1      0      ?      ?      ?      ?      1     0      0  ...   \n",
       "162      1     0      1      0      0      0      0      1     0      1  ...   \n",
       "163      1     0      1      1      0      1      1      1     1      1  ...   \n",
       "164      1     1      1      0      0      0      1      1     0      1  ...   \n",
       "\n",
       "    41.Alk 42.Prot 43.Crea 44.NNod 45.dnod 46.Bil 47.Iro 48.Oxy 49.Fer Class  \n",
       "0      150     7.1     0.7       1     3.5    0.5      ?      ?      ?     1  \n",
       "1        ?       ?       ?       1     1.8      ?      ?      ?      ?     1  \n",
       "2      109       7     2.1       5      13    0.1     28      6     16     1  \n",
       "3      174     8.1    1.11       2    15.7    0.2      ?      ?      ?     0  \n",
       "4      109     6.9     1.8       1       9      ?     59     15     22     1  \n",
       "..     ...     ...     ...     ...     ...    ...    ...    ...    ...   ...  \n",
       "160    109     7.6     0.7       5       3      ?      ?      ?      ?     1  \n",
       "161    280     6.7     0.7       1     2.2    2.3      ?      ?      ?     0  \n",
       "162    181     7.5    1.46       5    18.6      ?      ?      ?      ?     1  \n",
       "163    170     8.4    0.74       5      18      ?      ?      ?      ?     0  \n",
       "164    462     6.6    3.95       5     8.5   19.8      ?      ?      ?     0  \n",
       "\n",
       "[165 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcc_data     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the UCI website,\n",
    "this is an heterogeneous dataset, with 23 quantitative variables, \n",
    "and 26 qualitative variables. Overall, missing data represents 10.22% \n",
    "of the whole dataset and only eight patients have complete information \n",
    "in all fields (4.85%). The target variables is the survival at 1 year, \n",
    "and was encoded as a binary variable: 0 (dies) and 1 (lives). \n",
    "A certain degree of class-imbalance is also present (63 cases labeled as \"dies\" \n",
    "and 102 as \"alive\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy for creating a classification algorithm is to use something different from supervised learning. Supervised learning, of course, deals with creating algorithms to optimize a function, or a mapping/relationship between the input variables (or features), and output labels (1=life, 0=death). Some of thse optimizers include backpropagation (neural networks) and gradient descent (for linear, logistic regression). Random forests and decision trees are also supervised learning, but they don't use gradient optimization. Instead they use stuff called \"information-theory\" to create their function mappings.\n",
    "\n",
    "We won't be useing a direct supervised learning algorithm. Instead, after fixing missing values and balancing our dataset, we'll be using an optimizer called Expecgation-Maximization to create a generative model called a Gaussian Mixture model. We can then use this model to calculate the likelihood that a new unseen test-point is in class 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exploratory Data Analysis (EDA) –– Fixing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to fix some missing data; either we replace the missing one with the\n",
    "mean around the others, or drop it, or some other shit. We'll use Miria santos's method of the HEOM distance to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1.Gen', '2.Sym', '3.Alc', '4.HepB', '5.HepB', '6.HepB', '7.HepC',\n",
       "       '8.Cir', '9.End', '10.Smo', '11.Dia', '12.Obe', '13.Hem', '14.Art',\n",
       "       '15.CRen', '16.HIV', '17.Non', '18.EVar', '19.Spl', '20.PHyp', '21.Thr',\n",
       "       '22.LMet', '23.Rad', '24.Agedia', '25.Alcpd', '26.cigpy', '27.Sta',\n",
       "       '28.Encdeg', '29.Ascdeg', ' 30.IntNorRat', ' 31.Alp', ' 32.Hae',\n",
       "       ' 33.MCorVol', ' 34.Leu', '35.Plat', '36.Alb', '37.Bil', '38.Ala',\n",
       "       '39.Aspa', '40.Gam', '41.Alk', '42.Prot', '43.Crea', '44.NNod',\n",
       "       '45.dnod', '46.Bil', '47.Iro', '48.Oxy', '49.Fer', 'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcc_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, we have a feature data_type dictionary from the UCI repository \n",
    "nominal = 'nominal'\n",
    "ordinal = 'ordinal'\n",
    "integer = 'integer'\n",
    "continuous = 'continuous'\n",
    "ftypes = {0: nominal,\n",
    "1: nominal,\n",
    "2: nominal,\n",
    "3: nominal,\n",
    "4: nominal,\n",
    "5: nominal,\n",
    "6: nominal,\n",
    "7: nominal,\n",
    "8: nominal,\n",
    "9: nominal,\n",
    "10: nominal,\n",
    "11: nominal,\n",
    "12: nominal,\n",
    "13: nominal,\n",
    "14: nominal,\n",
    "15: nominal,\n",
    "16: nominal,\n",
    "17: nominal,\n",
    "18: nominal,\n",
    "19: nominal,\n",
    "20: nominal,\n",
    "21: nominal,\n",
    "22: nominal,\n",
    "23: integer,\n",
    "24: continuous,\n",
    "25: continuous,\n",
    "26: ordinal,\n",
    "27: ordinal,\n",
    "28: ordinal,\n",
    "29: continuous,\n",
    "30: continuous,\n",
    "31: continuous,\n",
    "32: continuous,\n",
    "33: continuous,\n",
    "34: continuous,\n",
    "35: continuous,\n",
    "36: continuous,\n",
    "37: continuous,\n",
    "38: continuous,\n",
    "39: continuous,\n",
    "40: continuous,\n",
    "41: continuous,\n",
    "42: continuous,\n",
    "43: integer,\n",
    "44: continuous,\n",
    "45: continuous,\n",
    "46: continuous,\n",
    "47: continuous,\n",
    "48: continuous,\n",
    "49: nominal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell programs the distance functions we use for fixing the missing values\n",
    "import math\n",
    "def HEOM_dist(rowA, rowB, df):\n",
    "    dist_vec = np.asarray(dist_list(rowA, rowB,df))\n",
    "    return np.linalg.norm(dist_vec)\n",
    "\n",
    "def dist_list(vecA, vecB, df):\n",
    "    L = []\n",
    "    for j in range(0,len(df.columns)-1):\n",
    "        # if the attribute is missing from vecA or vecB\n",
    "        if ((vecA[j] == '?') or (vecB[j] == '?')):\n",
    "            L.append(1)\n",
    "        else:\n",
    "            # if the attribute is there and the feature is discrete\n",
    "            if ((ftypes[j] == 'nominal') or (ftypes[j] == 'ordinal') or (ftypes[j] == 'integer')):\n",
    "                if (float(vecA[j]) == float(vecB[j])):\n",
    "                    L.append(0)\n",
    "                else:\n",
    "                    L.append(1)\n",
    "            # if the attribute is there and the feature is continuous\n",
    "            elif (ftypes[j] == 'continuous'):\n",
    "                numerator = abs(float(vecA[j])-float(vecB[j]))\n",
    "                # denominator is tricky. \n",
    "                # first make a copy of df.iloc[:,j]  \n",
    "                copy = df.iloc[:,j].copy().values\n",
    "                # then delete '?'\n",
    "                copy = np.delete(copy, np.where(copy=='?'))\n",
    "                # finally convert to float\n",
    "                copy = copy.astype(np.float)\n",
    "                denominator = max(copy)-min(copy)\n",
    "                ans = numerator/denominator\n",
    "                L.append(ans)\n",
    "    # return the list of distances (based on the coordinate, j)\n",
    "    return L        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f6b63f08644222b59a38251075762a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=165), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's fix the missing values!\n",
    "# for each row-vector v, check if there's missing values\n",
    "# if there is, we take distancees between v and the set of remaining rows\n",
    "    # we choose the row-vector w closest, and fill in the missing value\n",
    "# if v has no missing-values at all, we continue in the loop\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# initialize the new empty dataframe (0 rows, all columns)\n",
    "new_df = pd.DataFrame(columns=hcc_data.columns[:-1])\n",
    "\n",
    "for i in tqdm(hcc_data.index):\n",
    "    # each row \n",
    "    row_i = hcc_data.iloc[i, :][:-1]\n",
    "    if ('?' in row_i.values):\n",
    "        # if the row does have any missing values:\n",
    "        heom_distances = []\n",
    "        rest = hcc_data.index.delete(i)\n",
    "        for j in rest:\n",
    "            # jth-vector is row_j\n",
    "            row_j = hcc_data.iloc[j,:][:-1]\n",
    "            # before calculating HEOM, check row_j has complete info in the wanted features\n",
    "            # if row_j does, calculate HEOM. if not, skip over it.\n",
    "            # check where the '?' is in row_i, and if row_j has complete data in the same spots\n",
    "            row_i_missing = np.where(row_i.values == '?')[0]\n",
    "            if ('?' not in row_j.values[row_i_missing]):\n",
    "                # if row_j has no missing values in the same spots,\n",
    "                # then compute the HEOM distance\n",
    "                D = HEOM_dist(row_i, row_j, hcc_data)\n",
    "                heom_distances.append((j,D))\n",
    "            else:\n",
    "                # otherwise skip this vector and continue with the loop\n",
    "                continue\n",
    "                \n",
    "        # after the distances are computed and stored in a list:       \n",
    "        # closest vector to row_i according to distance D\n",
    "        closest_ind = min(heom_distances, key=lambda x: x[1])[0]\n",
    "        # we need to fill in the missing attributes in row_i from closest_v\n",
    "        closest_v = hcc_data.iloc[closest_ind, :][:-1].values # this is the row-vector in hcc_data\n",
    "        # replace the missing attributes from row_i with the corresponding ones from\n",
    "        # closest_v\n",
    "        # first we get the indices of the missing_value (the '?' value)\n",
    "        missing_indexes = np.where(row_i.values == '?')[0]\n",
    "        # then we get the corresponding values from closest_v\n",
    "        corresponding_vals = closest_v[missing_indexes.tolist()]\n",
    "        # finally we replace the values in row_i with the new corresponding values\n",
    "        row_i[missing_indexes] = corresponding_vals\n",
    "        # finally update the empty dataframe by adding the row\n",
    "        new_df = new_df.append(row_i, ignore_index=True)\n",
    "    else:\n",
    "        # if there is no missing attribute values in row_i\n",
    "        # just update the new_df dataframe by adding the row\n",
    "        new_df = new_df.append(row_i, ignore_index=True)\n",
    "        \n",
    "    time.sleep(0.25)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.Gen</th>\n",
       "      <th>2.Sym</th>\n",
       "      <th>3.Alc</th>\n",
       "      <th>4.HepB</th>\n",
       "      <th>5.HepB</th>\n",
       "      <th>6.HepB</th>\n",
       "      <th>7.HepC</th>\n",
       "      <th>8.Cir</th>\n",
       "      <th>9.End</th>\n",
       "      <th>10.Smo</th>\n",
       "      <th>...</th>\n",
       "      <th>41.Alk</th>\n",
       "      <th>42.Prot</th>\n",
       "      <th>43.Crea</th>\n",
       "      <th>44.NNod</th>\n",
       "      <th>45.dnod</th>\n",
       "      <th>46.Bil</th>\n",
       "      <th>47.Iro</th>\n",
       "      <th>48.Oxy</th>\n",
       "      <th>49.Fer</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>37</td>\n",
       "      <td>856</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>433</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>52.5</td>\n",
       "      <td>37</td>\n",
       "      <td>856</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>52.5</td>\n",
       "      <td>37</td>\n",
       "      <td>856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>59</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  1.Gen 2.Sym 3.Alc 4.HepB 5.HepB 6.HepB 7.HepC 8.Cir 9.End 10.Smo  ...  \\\n",
       "0     1     0     1      0      0      0      0     1     0      1  ...   \n",
       "1     0     0     0      0      0      0      1     1     0      0  ...   \n",
       "2     1     0     1      1      0      1      0     1     0      1  ...   \n",
       "3     1     1     1      0      0      0      0     1     0      1  ...   \n",
       "4     1     1     1      1      0      1      0     1     0      1  ...   \n",
       "\n",
       "  41.Alk 42.Prot 43.Crea 44.NNod 45.dnod 46.Bil 47.Iro 48.Oxy 49.Fer Class  \n",
       "0    150     7.1     0.7       1     3.5    0.5   52.5     37    856     1  \n",
       "1    433     6.5    0.87       1     1.8    0.4   52.5     37    856     1  \n",
       "2    109       7     2.1       5      13    0.1     28      6     16     1  \n",
       "3    174     8.1    1.11       2    15.7    0.2   52.5     37    856     0  \n",
       "4    109     6.9     1.8       1       9    0.1     59     15     22     1  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Class'] = hcc_data['Class']\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.Gen</th>\n",
       "      <th>2.Sym</th>\n",
       "      <th>3.Alc</th>\n",
       "      <th>4.HepB</th>\n",
       "      <th>5.HepB</th>\n",
       "      <th>6.HepB</th>\n",
       "      <th>7.HepC</th>\n",
       "      <th>8.Cir</th>\n",
       "      <th>9.End</th>\n",
       "      <th>10.Smo</th>\n",
       "      <th>...</th>\n",
       "      <th>41.Alk</th>\n",
       "      <th>42.Prot</th>\n",
       "      <th>43.Crea</th>\n",
       "      <th>44.NNod</th>\n",
       "      <th>45.dnod</th>\n",
       "      <th>46.Bil</th>\n",
       "      <th>47.Iro</th>\n",
       "      <th>48.Oxy</th>\n",
       "      <th>49.Fer</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>433.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>52.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>174.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>52.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1.Gen  2.Sym  3.Alc  4.HepB  5.HepB  6.HepB  7.HepC  8.Cir  9.End  10.Smo  \\\n",
       "0      1      0      1       0       0       0       0      1      0       1   \n",
       "1      0      0      0       0       0       0       1      1      0       0   \n",
       "2      1      0      1       1       0       1       0      1      0       1   \n",
       "3      1      1      1       0       0       0       0      1      0       1   \n",
       "4      1      1      1       1       0       1       0      1      0       1   \n",
       "\n",
       "   ...  41.Alk  42.Prot  43.Crea  44.NNod  45.dnod  46.Bil  47.Iro  48.Oxy  \\\n",
       "0  ...   150.0      7.1     0.70        1      3.5     0.5    52.5    37.0   \n",
       "1  ...   433.0      6.5     0.87        1      1.8     0.4    52.5    37.0   \n",
       "2  ...   109.0      7.0     2.10        5     13.0     0.1    28.0     6.0   \n",
       "3  ...   174.0      8.1     1.11        2     15.7     0.2    52.5    37.0   \n",
       "4  ...   109.0      6.9     1.80        1      9.0     0.1    59.0    15.0   \n",
       "\n",
       "   49.Fer  Class  \n",
       "0   856.0      1  \n",
       "1   856.0      1  \n",
       "2    16.0      1  \n",
       "3   856.0      0  \n",
       "4    22.0      1  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, before we do the matrix, we need to convert everything into float\n",
    "for col in new_df.columns:\n",
    "    new_df[col] = pd.to_numeric(new_df[col], errors='coerce')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking A Second Look at the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to select the top 10 features (columns) from the main set, since it has a total of 49 features, and honestly, keeping track of 49 features is *a lot* of information. Plus, creating generative models in 49 dimensions gets a bit tricky, and if you have a real-world dataset with even more features (also called *dimensions*, by the way) then it gets even harder to create models. But no one said machine learning would be easy!\n",
    "\n",
    "(*disclaimer: the \"best\" features, by the way is determined by something called \"mutual information\" between the predictor (independent) features and the label (0 or 1) but that is beyond this notebook and I honestly don't understand it enough to confidently explain at all.*)\n",
    "\n",
    "\n",
    "After we do this let's separate our reduced, 10-dimensional (10 features) dataset into a training and a testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we do anything let's set a global random seed for our ML algorithms\n",
    "np.random.seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use scikit SelectK Best features (using mutual information) to determine \"the best\"\n",
    "# K \"best\" parameters/features to select from the 49 parameters we have\n",
    "\n",
    "# if we can potentially use the THREE best features, we can plot the GMM in 3D space \n",
    "# as a great visual aid\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "K = 3\n",
    "selector = SelectKBest(mutual_info_classif, K)\n",
    "X = new_df.iloc[:, :-1]\n",
    "y = new_df.iloc[:, -1]\n",
    "X_reduced = selector.fit_transform(X,y) \n",
    "                                   \n",
    "features_selected = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11.Dia', ' 31.Alp', '41.Alk'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the best K features the computer selected!\n",
    "feature_names_selected = np.where(features_selected==True)[0].tolist()\n",
    "all_features = np.asarray(new_df.columns)\n",
    "all_features[feature_names_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the labels 0 and 1 are balanced. There are now 102 patient-survive records and 102 patient-death records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    102\n",
       "0     63\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 3), (90,), (75, 3), (75,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# im splitting the train/test as 55% train, 45% test. this is totally not standard.\n",
    "# im doing this to try and test out a Bayesian classifier vs. logistic regression,\n",
    "# and see how density estimation can be useful\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_reduced, y, \n",
    "                                                   train_size=0.55)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.622222\n",
       "0    0.377778\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_frequency = y_train.value_counts(normalize=True)\n",
    "class_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a use case, let's test out logistic regression on our dataset. Since we are using only 130 records for our training set, I don't expect logistic regression to have amazing predictive power on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use logistic regression as a test case model\n",
    "logreg = LogisticRegression(C=0.1, max_iter=1000, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logreg_preds = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cf_matrix_logreg = confusion_matrix(y_true=np.asarray(y_test), y_pred=logreg_preds)\n",
    "sns.heatmap(cf_matrix_logreg/np.sum(cf_matrix_logreg), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix based on Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And....we weren't wrong. While logistic regression kind of nails the false-negative rate at about 11%, its false-positive error rate is 22%, which means that for every 5 new patients with hepatocellular carcinoma, 1 of them would be predicted to survive carcinoma, while their actual chance of survival would be very low. This kind of false-positive error rate should be throughly scrutinized in the context of healthcare patient data related to survival. This is one of the reasons why we'll be looking into generative models to tackle small datasets for the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: A Generative Classifier\n",
    "\n",
    "\n",
    "We can do this data analysis a different way. Instead of creating a fully predictive model with logistic regression (or say, a neural network) what if –– we do a non-parametric approach instead?\n",
    "\n",
    "Let's take a look at the following equation, which states Bayes' Rule of Inference, but for two classification hypotheses we want to test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{P(H_1|Data)}{P(H_2|Data)} = \\frac{f(Data|H_1)}{f(Data|H_2)}\\frac{P(H_1)}{P(H_2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation basically says, that the odds of Hypothesis 1 being true after seeing the new data point, D, over the odds of Hypothesis 2 being true, is the ratio of how likely D fits hypothesis 1 over hypothesis 2 (symbolized by \"f\"), times the prior odds we believe that hypothesis 1 is right over hypothesis 2.\n",
    "\n",
    "The part we call \"f\" is called the *likelihood* of the data fitting the hypothesis H1 or H2. It's usually difficult to calculate this, and we use the lowercase \"f\" instead of probability \"P\" because there are so many data points in our set, we consider the set *continuous* instead, and use \"f\". You can think of $f(x)$ as a *density* –– it gives us a map of how likely certain data points would be *given* that either hypothesis 1 or 2 is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficult part is constructing $f(x)$. This would usually be part of \"unsupervised learning\", where the computer is traianed to find patterns in data without any labels. You might of heard, for example, K-Means – which clusters data into K different blobs based on the \"centroid\" of each cluster. \n",
    "\n",
    "Gaussian Mixture Modeling (GMM) is a general version of K-Means, and while it can be used for clustering, it has waaaay more power than that. Because GMM is a model, a *probabilistic model*, it basically tells us a whole bunch of info about how dense the data points are, and how probable is certain parts of the data from other parts. and one of the amazing things about GMM is that its a *generative model*, which means you can generate *new* samples of the data from your model pretty easily by just calling the .sample method in Sci-kit Learn. This would be pretty hard to do with K-means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11.Dia</th>\n",
       "      <th>31.Alp</th>\n",
       "      <th>41.Alk</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.04</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44340.00</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2159.00</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   11.Dia    31.Alp  41.Alk  Class\n",
       "0     1.0      5.04   165.0      1\n",
       "1     0.0  44340.00   462.0      0\n",
       "2     0.0      2.10   127.0      0\n",
       "3     1.0      7.00   161.0      1\n",
       "4     1.0   2159.00   335.0      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next we are going to separate the transformed dataset into either life (1) or death (0)\n",
    "# class labels\n",
    "#the shrunk dataset with only the bst 20 features is called \"short_df\"\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "short_df = pd.DataFrame(X_train, \n",
    "                        columns=all_features[feature_names_selected])\n",
    "\n",
    "short_df['Class'] = y_train\n",
    "short_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56\n",
       "0    34\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56, 4), (34, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_df_life = short_df[short_df['Class']==1]\n",
    "short_df_death = short_df[short_df['Class']==0]\n",
    "\n",
    "short_df_life.shape, short_df_death.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to set up the Gaussian Mixture generative model. First we need to import it, and make an object instance of the class. And for the generative model, we are *not* going to touch the testing data; we'll just be dealing with the training set.\n",
    "\n",
    "A question you might ask is: how many components do we need to use in the model? It's a great question! And the answer is often: we don't know beforehand. So instead of running 1 generative model, we are going to run 15 of them each model using from 1 componenent to N components. Once we do this, we can use a number called the *Akaike Information Criterion (AIC)* (don't ask me about it!) to determine how many clusters we should use. A good way to do this is called the \"elbow method\", where we look at the plotted graph of cluster number vs. AIC (it should look like a bent elbow where the bend is at the bottom), and we pick the cluster number closest to the bend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56, 3), (34, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_life = short_df_life.iloc[:, :-1]\n",
    "X_train_death = short_df_death.iloc[:, :-1]\n",
    "X_train_life.shape, X_train_death.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to apply density estimation as a predictive tool, ket's go back to the Bayesian inference hypothesis equation above. Because we balanced the class labels in our dataset, the quotient $\\frac{P(H_1)}{P(H_2)}$ is 1 because the ratio of life-labels to death-labels is the same. So we only have to worry about the likelihood ratio. \n",
    "\n",
    "Dividing numbers in Python can give weird errors, so instead, we'll work with the *logarithm* of the likelihood ratio, written as:\n",
    "\n",
    "$$log(\\frac{P(Data|H_1)}{P(Data|H_2)})$$\n",
    "\n",
    "which can be written as:\n",
    "\n",
    "$$delta = log(P(Data|H_1) - log(P(Data|H_2))$$\n",
    "\n",
    "The numerator we'll call *life_log_prob* and the denominator as *death_log_prob*.\n",
    "\n",
    "Our key calculation is this: in the quotient of likelihoods, if the quotient is > 1, then the likelihood of the test data point under hypothesis 1 (class label=1) is higher than class label 0, so we classify it as \"survival\". Otherwise, it's classified as \"death\". When we take the logarithm of the likelihood ratio, what this classification rule translates to is that we classify the new data point as \"survival\" if the difference, $delta$ > 0, and \"death\" if $delta$ < 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_life)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also do kernel density estimation\n",
    "# we're gonna be using grid search and cross-validation to find the BEST bandwidth value\n",
    "# for each KDE estimate of the training data (based on class label)\n",
    "# we'll be using K-fold cross validation\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rabeya/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/rabeya/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "bandwidths = 10 ** np.linspace(-1, 2, 500)\n",
    "\n",
    "# first the survival data\n",
    "grid_life = GridSearchCV(  KernelDensity(kernel='gaussian'), {'bandwidth': bandwidths}, cv=5   )\n",
    "grid_life.fit(X_train_life);\n",
    "\n",
    "\n",
    "# then the death data :(\n",
    "grid_death = GridSearchCV(  KernelDensity(kernel='gaussian'), {'bandwidth': bandwidths}, cv=5   )\n",
    "grid_death.fit(X_train_death);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bandwidth': 10.0}, {'bandwidth': 10.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_life.best_params_, grid_death.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and fit the KDE models to the survival and death data\n",
    "KDE_life = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
    "KDE_death = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
    "\n",
    "# fit the models\n",
    "KDE_life.fit(X_train_life)\n",
    "KDE_death.fit(X_train_death)\n",
    "\n",
    "\n",
    "\n",
    "# score_samples returns the log of the probability density\n",
    "# before we \n",
    "logprob = kde.score_samples(x_d[:, None])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
